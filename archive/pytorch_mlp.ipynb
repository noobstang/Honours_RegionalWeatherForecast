{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRoCCINQCq_h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas import read_csv, DataFrame, concat\n",
        "from statsmodels.tsa.stattools import acf, q_stat, adfuller, kpss\n",
        "import csv\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoSJ8rFE8SGF"
      },
      "outputs": [],
      "source": [
        "train_res_lst = []\n",
        "test_res_lst = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1hlLcm38SGG"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper():\n",
        "    def __init__(self, patience=5, min_delta=0, filename='optimal_weight.pth'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_val_loss = np.inf\n",
        "        self.filename = filename\n",
        "\n",
        "    def early_stop(self, model, val_loss):\n",
        "        if val_loss < self.min_val_loss:\n",
        "            self.min_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), self.filename)\n",
        "            self.counter = 0\n",
        "        elif val_loss > (self.min_val_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                model.load_state_dict(torch.load(self.filename))\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shyNjxLSC3M4"
      },
      "outputs": [],
      "source": [
        "class MLP_Model(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_features, seq_length):\n",
        "        super(MLP_Model, self).__init__()\n",
        "        self.input_layer = nn.Linear(dim_features*seq_length, 100, bias=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.hidden_layer_1 = nn.Linear(100, 8, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.hidden_layer_2 = nn.Linear(8, 50, bias=False)\n",
        "        self.output_layer = nn.Linear(50, 1, bias=False)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.train_loss_lst = []\n",
        "        self.test_loss_lst = []\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.input_layer(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.hidden_layer_1(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        # in the following part, it will be replaced by the dlem model\n",
        "        x = self.hidden_layer_2(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def dense_forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.input_layer(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.hidden_layer_1(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, train_loader, test_loader, early_stopper, epochs, learning_rate=1e-4):\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        criterion = torch.nn.MSELoss()\n",
        "\n",
        "        for ix in range(epochs):\n",
        "            tmp_train_loss = []\n",
        "            tmp_test_loss = []\n",
        "            for inputs, val in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = self.forward(inputs)\n",
        "                loss = criterion(val, output)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tmp_train_loss.append(loss.item())\n",
        "\n",
        "            for test_inputs, val in test_loader:\n",
        "                output = self.forward(test_inputs)\n",
        "                test_loss = criterion(val, output)\n",
        "                tmp_test_loss.append(test_loss.item())\n",
        "\n",
        "\n",
        "            print(\"Epoch \", ix+1, \"Train Loss : \", np.mean(tmp_train_loss), \\\n",
        "                  \" , Test Loss : \", np.mean(tmp_test_loss))\n",
        "\n",
        "            self.train_loss_lst.append(np.mean(tmp_train_loss))\n",
        "            self.test_loss_lst.append(np.mean(tmp_test_loss))\n",
        "\n",
        "            if early_stopper.early_stop(self, np.mean(tmp_test_loss)):\n",
        "                print(\"early stopping is true\")\n",
        "                return self.train_loss_lst, self.test_loss_lst\n",
        "\n",
        "        return self.train_loss_lst, self.test_loss_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Lxto22D_oZ"
      },
      "outputs": [],
      "source": [
        "class Scaler():\n",
        "    def __init__(self):\n",
        "        super(Scaler, self).__init__()\n",
        "        # from the left to the right, it is doy, fh2o, ta, h2o, sm, st, n2o\n",
        "        self.min_val = torch.tensor([[170,  0, 0,  0, 30, 10, 0]])\n",
        "        self.max_val = torch.tensor([[295,  5, 40, 50, 50, 25, 260]])\n",
        "\n",
        "    def reverse_transform(self, input_data):\n",
        "        output_data = (self.max_val - self.min_val) * input_data + self.min_val\n",
        "        return output_data\n",
        "\n",
        "    def transform(self, input_data):\n",
        "        output_data = (input_data - self.min_val)/(self.max_val - self.min_val)\n",
        "        return output_data\n",
        "\n",
        "    def exponential_transform(self, input_data, alpha):\n",
        "        return torch.exp(alpha*input_data)\n",
        "\n",
        "    def reverse_n2o(self, n2o_predicted):\n",
        "        n2o_output = (self.max_val[:, -1] - self.min_val[:, -1]) * n2o_predicted + self.min_val[:, -1]\n",
        "        return n2o_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE9PcehI8SGI"
      },
      "outputs": [],
      "source": [
        "def adf_test(series,title=''):\n",
        "    \"\"\"\n",
        "    Pass in a time series and an optional title, returns an ADF report\n",
        "    \"\"\"\n",
        "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
        "    result = adfuller(series, autolag='t-stat', maxlag = 100, regression='n') # .dropna() handles differenced data\n",
        "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
        "    out = pd.Series(result[0:4],index=labels)\n",
        "    #print(\"Result : \", result)\n",
        "    for key,val in result[4].items():\n",
        "        out[f'critical value ({key})']=val\n",
        "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
        "    if result[1] <= 0.005:\n",
        "        print(\"Strong evidence against the null hypothesis\")\n",
        "        print(\"Reject the null hypothesis\")\n",
        "        print(\"Data has no unit root and is stationary\")\n",
        "    else:\n",
        "        print(\"Weak evidence against the null hypothesis\")\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "        print(\"Data has a unit root and is non-stationary\")\n",
        "\n",
        "\n",
        "def kpss_test(timeseries, title=''):\n",
        "    print(f'Results of KPSS Test for {title} : ')\n",
        "    kpsstest = kpss(timeseries, regression=\"ct\", nlags=500)\n",
        "    kpss_output = pd.Series(\n",
        "        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n",
        "    )\n",
        "    for key, value in kpsstest[3].items():\n",
        "        kpss_output[\"Critical Value (%s)\" % key] = value\n",
        "    print(kpss_output.to_string())\n",
        "\n",
        "    if kpsstest[1] <= 0.05:\n",
        "        print(\"Strong evidence against the null hypothesis\")\n",
        "        print(\"Reject the null hypothesis\")\n",
        "        print(\"Data is not trend stationary\")\n",
        "    else:\n",
        "        print(\"Weak evidence against the null hypothesis\")\n",
        "        print(\"Fail to reject the null hypothesis\")\n",
        "        print(\"Data is trend stationary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiO7xTp28SGJ"
      },
      "outputs": [],
      "source": [
        "def calculate_metric(original, predicted, title):\n",
        "    print(title)\n",
        "    rmse = mean_squared_error(original, predicted, squared=False)\n",
        "    print('Test RMSE: %.6f' % rmse)\n",
        "    mae = mean_absolute_error(original, predicted)\n",
        "    print('Test MAE: %.6f' % mae)\n",
        "    mape = mean_absolute_percentage_error(original, predicted)\n",
        "    print('Test MAPE: %.6f' % mape)\n",
        "    r2 = r2_score(original, predicted)\n",
        "    print('Test R2: %.6f' % r2)\n",
        "    residue_err = original - predicted\n",
        "    print(\"--------------\")\n",
        "    adf_test(residue_err, title)\n",
        "    kpss_test(residue_err, title)\n",
        "    print(\"--------------\")\n",
        "    print('Test Residue Mean: %.6f' %np.mean(residue_err))\n",
        "    print('Test Residue std: %.6f' %np.std(residue_err))\n",
        "    print('Test Residue var: %.6f' %np.var(residue_err))\n",
        "\n",
        "    acc_rate = np.sum(predicted)/np.sum(original)\n",
        "    print(\"Sum of the Emission FN2O (Measured) : %6f\" %np.sum(original))\n",
        "    print(\"Sum of the Emission FN2O (Predicted): %.6f\" %np.sum(predicted))\n",
        "    print(\"The percentage of the accuracy rate (p/m): %6f\" %acc_rate)\n",
        "\n",
        "    return np.array([rmse, mae, mape, r2, acc_rate])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt3e1KtHDxRd"
      },
      "outputs": [],
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "s-kY7saJD0vR",
        "outputId": "49b40fde-572e-4eb2-8295-eb53d46b15ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0c8287f36d43>:3: DtypeWarning: Columns (46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  broadcast_side_2021 = pd.read_csv(\"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Unnamed: 0'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0c8287f36d43>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#broadcast_side_2021 = pd.read_csv(\"2021_broadcast_side_all.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbroadcast_side_2021\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbroadcast_side_2021\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#broadcast_side_2021.drop('DOY', axis=1, inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbroadcast_side_2021\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_side_2021\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DOY'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FH2O'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'H2O'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FN2O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5397\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5398\u001b[0m         \"\"\"\n\u001b[0;32m-> 5399\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5400\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5401\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4505\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4545\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4546\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4547\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6934\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6935\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6936\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
          ]
        }
      ],
      "source": [
        "# prepare the training data\n",
        "#broadcast_side_2021 = pd.read_csv(\"2021_broadcast_side_all.csv\")\n",
        "broadcast_side_2021 = pd.read_csv(\"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\")\n",
        "broadcast_side_2021.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "#broadcast_side_2021.drop('DOY', axis=1, inplace=True)\n",
        "broadcast_side_2021 = broadcast_side_2021[['DOY','FH2O', 'TA', 'H2O', 'SM', 'ST', 'FN2O']]\n",
        "print(broadcast_side_2021[500:4500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELHOQOhi8SGM"
      },
      "outputs": [],
      "source": [
        "# prepare the training data\n",
        "broadcast_side_2022 = pd.read_csv(\"2022_broadcast_side_all.csv\")\n",
        "broadcast_side_2022.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "#broadcast_side_2021.drop('DOY', axis=1, inplace=True)\n",
        "broadcast_side_2022 = broadcast_side_2022[['DOY', 'FH2O','TA', 'H2O', 'SM', 'ST', 'FN2O']]\n",
        "broadcast_side_2022['DOY'] = broadcast_side_2022['DOY']\n",
        "print(broadcast_side_2022[0:4000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsgXCQlH8SGM"
      },
      "outputs": [],
      "source": [
        "scaler = Scaler()\n",
        "\n",
        "test_data = scaler.transform(torch.tensor(broadcast_side_2022[0:4000].values))\n",
        "train_data = scaler.transform(torch.tensor(broadcast_side_2021[500:4500].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdQ6cBce8SGN"
      },
      "outputs": [],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxaRZmvz8SGO"
      },
      "outputs": [],
      "source": [
        "test_n2o = scaler.exponential_transform(train_data[:, 0], alpha=-3) - 0.07\n",
        "train_n2o = scaler.exponential_transform(test_data[:, 0], alpha=-3)\n",
        "\n",
        "# plot history\n",
        "plt.figure(figsize=(18, 12))\n",
        "#plt.plot(res.detach().numpy(), label='train actual')\n",
        "#plt.plot(train_y, label='train predict')\n",
        "\n",
        "plt.plot(test_n2o, label='test actual')\n",
        "plt.plot(train_n2o, label='train actual')\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Nitrate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGphuQUE8SGP"
      },
      "outputs": [],
      "source": [
        "test_data[:,0] = test_n2o\n",
        "\n",
        "train_data[:,0] = train_n2o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzhNKbbiEAxC"
      },
      "outputs": [],
      "source": [
        "# specify the number of lag steps\n",
        "# prepare the training data for inputting the LSTM model\n",
        "n_steps = 5\n",
        "n_features = 6\n",
        "n_out = 1\n",
        "train_x = series_to_supervised(train_data[:, 0:6], n_steps)\n",
        "test_x = series_to_supervised(test_data[:, 0:6], n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuM1DIu_8SGP"
      },
      "outputs": [],
      "source": [
        "train_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhL0M9lgENRk"
      },
      "outputs": [],
      "source": [
        "train_x = train_x.reshape(train_x.shape[0], n_steps, n_features)\n",
        "test_x = test_x.reshape(test_x.shape[0], n_steps, n_features)\n",
        "train_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBHnOCGU8SGQ"
      },
      "outputs": [],
      "source": [
        "y_2021_f = train_data[:, -1]\n",
        "y_2022_f = test_data[:, -1]\n",
        "y_2021 = y_2021_f[n_steps:]\n",
        "y_2022 = y_2022_f[n_steps:]\n",
        "y_2021 = y_2021.reshape(4000-n_steps, 1)\n",
        "y_2022 = y_2022.reshape(4000-n_steps, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjZIt0DG8SGR"
      },
      "outputs": [],
      "source": [
        "test_y = broadcast_side_2022[n_steps:4000]['FN2O']\n",
        "train_y = broadcast_side_2021[500+n_steps:4500]['FN2O']\n",
        "\n",
        "test_y = test_y.values.reshape(4000-n_steps)\n",
        "train_y = train_y.values.reshape(4000-n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1CYWNDR8SGR"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "dataset_2021 = TensorDataset(torch.tensor(train_x, dtype=torch.float32), \\\n",
        "                           torch.tensor(y_2021, dtype=torch.float32))\n",
        "loader_2021 = DataLoader(dataset_2021, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "dataset_2022 = TensorDataset(torch.tensor(test_x, dtype=torch.float32), \\\n",
        "                           torch.tensor(y_2022, dtype=torch.float32))\n",
        "loader_2022 = DataLoader(dataset_2022, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SnX-XDbE7k7"
      },
      "outputs": [],
      "source": [
        "mlp = MLP_Model(n_features, n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xhc8peu8SGT"
      },
      "outputs": [],
      "source": [
        "# Print model's state_dict\n",
        "#print(\"Generator's state_dict:\")\n",
        "for param_tensor in mlp.state_dict():\n",
        "    print(param_tensor, \"\\t\", mlp.state_dict()[param_tensor].size())\n",
        "\n",
        "sum(p.numel() for p in mlp.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRuWmTsl8SGT"
      },
      "outputs": [],
      "source": [
        "early_stopper = EarlyStopper(20, 0, filename='mlp_optimal_weight.pth')\n",
        "train_lss, test_lss = mlp.train(loader_2021, loader_2022, early_stopper, epochs=100, learning_rate=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkpneaZv8SGU"
      },
      "outputs": [],
      "source": [
        "res_2022_n = mlp.forward(torch.tensor(test_x).float())\n",
        "res_2021_n = mlp.forward(torch.tensor(train_x).float())\n",
        "res_2022 = scaler.reverse_n2o(res_2022_n).reshape(4000-n_steps)\n",
        "res_2021 = scaler.reverse_n2o(res_2021_n).reshape(4000-n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFYPKFRYnlU0"
      },
      "outputs": [],
      "source": [
        "width = 40\n",
        "height = 33\n",
        "plt.rcParams.update({'font.size': 50})\n",
        "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(width, height))\n",
        "\n",
        "ax[0].plot(res_2022.detach().numpy(), color='orangered', label='2022 Predicted Emission')\n",
        "ax[0].plot(test_y, color='cyan', label='2022 Measured Emission')\n",
        "ax[0].legend()\n",
        "ax[0].set_title(\"Flux of N2O for 2022 Growing Season (MLP, 5 Steps)\")\n",
        "ax[0].set_xlabel(\"Time Steps\")\n",
        "ax[0].set_ylabel(\"$nmol \\ m^{-2} s^{-1}$\")\n",
        "\n",
        "\n",
        "ax[1].plot(res_2021.detach().numpy(), color='orangered', label='2021 Predicted Emission')\n",
        "ax[1].plot(train_y, color='cyan', label='2021 Measured Emission')\n",
        "ax[1].legend()\n",
        "ax[1].set_title(\"Flux of N2O for 2021 Growing Season (MLP, 5 Steps)\")\n",
        "ax[1].set_xlabel(\"Time Steps\")\n",
        "ax[1].set_ylabel(\"$nmol \\ m^{-2} s^{-1}$\")\n",
        "\n",
        "\n",
        "f.tight_layout()\n",
        "f.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cHAhGJ88SGU"
      },
      "outputs": [],
      "source": [
        "train_result = calculate_metric(train_y, res_2021.detach().numpy(), \"2021 growing season (MLP, 5 Steps)\")\n",
        "train_res_lst.append(train_result)\n",
        "print(\"****************************************************************************\")\n",
        "test_result = calculate_metric(test_y, res_2022.detach().numpy(), \"2022 growing season (MLP, 5 Steps)\")\n",
        "test_res_lst.append(test_result)\n",
        "print(\"-----------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvXk4BU18SGV"
      },
      "outputs": [],
      "source": [
        "#wfp, P_clay, Rh, kden, temp,  k_nit, D_NO3, D_NH4\n",
        "dense_test  = mlp.dense_forward(torch.tensor(test_x).float()).detach().numpy()\n",
        "\n",
        "dense_train  = mlp.dense_forward(torch.tensor(train_x).float()).detach().numpy()\n",
        "\n",
        "\n",
        "width = 100\n",
        "height = 33\n",
        "plt.rcParams.update({'font.size': 50})\n",
        "f, ax = plt.subplots(nrows=2, ncols=4, figsize=(width, height))\n",
        "\n",
        "ax[0][0].plot(dense_train[:,0], color='orangered', label='Training Dataset')\n",
        "ax[0][0].plot(dense_test[:,0], color='cyan', label='Testing Dataset')\n",
        "ax[0][0].legend()\n",
        "ax[0][0].set_title(\"Output of Node 1\")\n",
        "ax[0][0].set_xlabel(\"Time Steps\")\n",
        "\n",
        "\n",
        "ax[0][1].plot(dense_train[:,1], color='orangered', label='Training Dataset')\n",
        "ax[0][1].plot(dense_test[:,1], color='cyan', label='Testing Dataset')\n",
        "ax[0][1].legend()\n",
        "ax[0][1].set_title(\"Output of Node 2\")\n",
        "ax[0][1].set_xlabel(\"Time Steps\")\n",
        "\n",
        "ax[0][2].plot(dense_train[:,2], color='orangered', label='Training Dataset')\n",
        "ax[0][2].plot(dense_test[:,2], color='cyan', label='Testing Dataset')\n",
        "ax[0][2].legend()\n",
        "ax[0][2].set_title(\"Output of Node 3\")\n",
        "ax[0][2].set_xlabel(\"Time Steps\")\n",
        "\n",
        "\n",
        "ax[0][3].plot(dense_train[:,3], color='orangered', label='Training Dataset')\n",
        "ax[0][3].plot(dense_test[:,3], color='cyan', label='Testing Dataset')\n",
        "ax[0][3].legend()\n",
        "ax[0][3].set_title(\"Output of Node 4\")\n",
        "ax[0][3].set_xlabel(\"Time Steps\")\n",
        "\n",
        "\n",
        "ax[1][0].plot(dense_train[:,4], color='orangered', label='Training Dataset')\n",
        "ax[1][0].plot(dense_test[:,4], color='cyan', label='Testing Dataset')\n",
        "ax[1][0].legend()\n",
        "ax[1][0].set_title(\"Output of Node 5\")\n",
        "ax[1][0].set_xlabel(\"Time Steps\")\n",
        "\n",
        "ax[1][1].plot(dense_train[:,5], color='orangered', label='Training Dataset')\n",
        "ax[1][1].plot(dense_test[:,5], color='cyan', label='Testing Dataset')\n",
        "ax[1][1].legend()\n",
        "ax[1][1].set_title(\"Output of Node 6\")\n",
        "ax[1][1].set_xlabel(\"Time Steps\")\n",
        "\n",
        "ax[1][2].plot(dense_train[:,6], color='orangered', label='Training Dataset')\n",
        "ax[1][2].plot(dense_test[:,6], color='cyan', label='Testing Dataset')\n",
        "ax[1][2].legend()\n",
        "ax[1][2].set_title(\"Output of Node 7\")\n",
        "ax[1][2].set_xlabel(\"Time Steps\")\n",
        "\n",
        "ax[1][3].plot(dense_train[:,7], color='orangered', label='Training Dataset')\n",
        "ax[1][3].plot(dense_test[:,7], color='cyan', label='Testing Dataset')\n",
        "ax[1][3].legend()\n",
        "ax[1][3].set_title(\"Output of Node 8\")\n",
        "ax[1][3].set_xlabel(\"Time Steps\")\n",
        "\n",
        "f.tight_layout()\n",
        "f.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orDEpeE28SGV"
      },
      "outputs": [],
      "source": [
        "print(\"2021 Train Res (MLP, 5 Steps) : \", train_res_lst)\n",
        "print(\"Train Mean : \", np.mean(train_res_lst, axis=0))\n",
        "print(\"Train Standard : \", np.std(train_res_lst, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjGHgqnt8SGV"
      },
      "outputs": [],
      "source": [
        "print(\"2022 Test Res (MLP, 5 Steps): \", test_res_lst)\n",
        "print(\"Test Mean : \", np.mean(test_res_lst, axis=0))\n",
        "print(\"Test Standard : \", np.std(test_res_lst, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uayv1XgO8SGW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}