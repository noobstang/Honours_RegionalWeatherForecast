{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Weather Forecasting with 4 features using historical data via TDNN"
      ],
      "metadata": {
        "id": "15Cg3bhGBbRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Required Import Statements\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n"
      ],
      "metadata": {
        "id": "MFJtjBeNsCjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link to 6 weather datasets\n",
        "url_ottawa = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\"\n",
        "url_ottawa_south = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawasouth_daily.csv\"\n",
        "url_gatineau = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_gatineau_daily.csv\"\n",
        "url_chelsea = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_chelsea_daily.csv\"\n",
        "url_kemptville = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_kemptville_daily.csv\"\n",
        "url_renfrew = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_renfrew_daily.csv\"\n",
        "\n",
        "# Load and Preprocess Data\n",
        "url = url_ottawa\n",
        "#url = 'https://raw.githubusercontent.com/Sattar-A/HonoursProject_CSI4900/main/data/weatherstats_ottawa_daily.csv?token=GHSAT0AAAAAACNBWJ7MI3IZIOPLGQ6OLTU4ZPPCOYA'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# data processing transformation\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "filtered_data = data[(data['date'].dt.month >= 5) & (data['date'].dt.month <= 11)]  # restrict data range from May to November\n",
        "filtered_data = filtered_data[(filtered_data['date'].dt.year >= 2013) & (filtered_data['date'].dt.year <= 2023)]  # overall data range from years 2013-2023\n",
        "selected_columns = ['avg_hourly_temperature', 'precipitation', 'solar_radiation', 'avg_hourly_pressure_station']  # set 4 features for input and output\n",
        "final_data = filtered_data[selected_columns]\n",
        "# test data processing transformation\n",
        "filtered_data_test = filtered_data[(filtered_data['date'].dt.year == 2023)] # data from 2023 only\n",
        "test_data = filtered_data_test[selected_columns]\n",
        "\n",
        "#window size\n",
        "window_size = 14\n",
        "pred_size = 1\n",
        "\n",
        "# Handle Missing Values\n",
        "#final_data = final_data.fillna(method='ffill')  # option 1: forward fill\n",
        "final_data = final_data.dropna()  # option 2: drop data with null values\n",
        "\n",
        "# Normalize the Data\n",
        "scaler = MinMaxScaler()  # MinMax scaler\n",
        "scaled_data = scaler.fit_transform(final_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkYF__HTzxMj",
        "outputId": "cfd9dd79-9e18-4822-93e2-696b0efe9b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-5b9477e2d748>:12: DtypeWarning: Columns (46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(url)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# URL to the dataset\n",
        "url_ottawa = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\"\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv(url_ottawa)\n",
        "\n",
        "# Step 2: Filter data based on the specified date range and months\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "filtered_data = data[(data['date'].dt.month >= 5) & (data['date'].dt.month <= 11)]\n",
        "filtered_data = filtered_data[(filtered_data['date'].dt.year >= 2013) & (filtered_data['date'].dt.year <= 2023)]\n",
        "\n",
        "# Step 3: Select the relevant columns\n",
        "selected_columns = ['avg_hourly_temperature', 'precipitation', 'solar_radiation', 'avg_hourly_pressure_station']\n",
        "final_data = filtered_data[selected_columns]\n",
        "\n",
        "# Step 4: Handle missing values\n",
        "final_data = final_data.dropna()  # Option 2: drop data with null values\n",
        "\n",
        "# Step 5: Normalize the dataset\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(final_data)\n",
        "\n",
        "# Convert scaled_data to a DataFrame (optional, if you need to inspect or use it further in DataFrame format)\n",
        "#scaled_data_df = pd.DataFrame(scaled_data, columns=selected_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbPubUmRsCYO",
        "outputId": "f451d334-2cf7-44c1-ee65-565397a08d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-6ef46c74ae40>:5: DtypeWarning: Columns (46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(url_ottawa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "kNfTZS1nsisM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader for Training\n",
        "def create_sequences(data, sequence_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        x = data[i:i+sequence_length]\n",
        "        y = data[i+sequence_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return torch.FloatTensor(xs), torch.FloatTensor(ys)"
      ],
      "metadata": {
        "id": "3yrjurAX0Oky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert scaled_data to PyTorch tensors\n",
        "scaled_data_tensor = torch.FloatTensor(scaled_data).reshape(-1, len(selected_columns))\n",
        "\n",
        "# Assuming the sequence length for the model is defined\n",
        "# Here you need to ensure that the data is correctly reshaped for your model's input.\n",
        "# For instance, if your model expects inputs shaped as (batch_size, sequence_length, number_of_features):\n",
        "sequence_length = 14  # This is an example value; adjust based on your model's expected input\n",
        "batch_size = scaled_data_tensor.shape[0] // sequence_length\n",
        "\n",
        "if scaled_data_tensor.shape[0] % sequence_length == 0:\n",
        "    # If the total number of data points is perfectly divisible by the sequence_length\n",
        "    reshaped_data = scaled_data_tensor.reshape(batch_size, sequence_length, len(selected_columns))\n",
        "else:\n",
        "    # Handle cases where the total number of data points is not perfectly divisible by the sequence_length\n",
        "    # This might involve trimming some data points or adjusting your approach\n",
        "    reshaped_data = scaled_data_tensor[:- (scaled_data_tensor.shape[0] % sequence_length)].reshape(batch_size, sequence_length, len(selected_columns))\n",
        "\n",
        "# Proceed with using reshaped_data as input to your model or data preparation steps\n",
        "\n",
        "\n",
        "# Assuming 'scaled_data' from the preprocessing steps\n",
        "# Convert scaled data to PyTorch tensors\n",
        "#scaled_data_tensor = torch.FloatTensor(scaled_data).view(-1)\n",
        "\n",
        "\n",
        "\n",
        "# window size 14 * 4 features = 56\n",
        "sequence_length = window_size * len(selected_columns)\n",
        "X, y = create_sequences(reshaped_data, sequence_length)\n",
        "\n",
        "# Split the data (adjust based on the actual train/test split by date)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the model\n",
        "class TDNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(TDNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Model parameters\n",
        "hidden_size = 128\n",
        "model = TDNN(sequence_length, hidden_size, len(selected_columns))\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    for seq, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(seq)\n",
        "        loss = criterion(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "II15-POyr20t",
        "outputId": "887b1ca5-aae5-4618-cfeb-133264380d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "only one element tensors can be converted to Python scalars",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ae72c1a2a219>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# window size 14 * 4 features = 56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#sequence_length = window_size * len(selected_columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Split the data (adjust based on the actual train/test split by date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-66ef386b0957>\u001b[0m in \u001b[0;36mcreate_sequences\u001b[0;34m(data, sequence_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "ZjSylXvF5peQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Assuming 'final_data' is the DataFrame after preprocessing, and 'scaler' is already fitted\n",
        "# Filter to get 2022 data\n",
        "data_2022 = final_data[(final_data['date'].dt.year == 2022)]\n",
        "\n",
        "# Assuming 'data_2022' does not include the 'date' column after selection; if it does, drop or ignore it in the scaling process\n",
        "# Select the last 14 days of 2022\n",
        "last_14_days = data_2022.tail(14)\n",
        "\n",
        "# Scale the data\n",
        "# Ensure the 'scaler' has been fitted on the training dataset to avoid data leakage\n",
        "last_14_days_scaled = scaler.transform(last_14_days[selected_columns])\n",
        "\n",
        "# Convert to PyTorch tensor and reshape to match the model input\n",
        "# Here, we're assuming each feature from the last 14 days is a separate input to the model,\n",
        "# thus needing to reshape to (1, sequence_length * number_of_features)\n",
        "last_window_2022 = torch.tensor(last_14_days_scaled, dtype=torch.float).view(1, -1)"
      ],
      "metadata": {
        "id": "vohunfZi12qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_window_predictions(model, initial_window, n_predictions, scaler):\n",
        "    \"\"\"\n",
        "    Generate predictions using a rolling window approach.\n",
        "\n",
        "    :param model: The trained PyTorch model.\n",
        "    :param initial_window: The last known data window to start predictions.\n",
        "    :param n_predictions: Number of future time steps (days) to predict.\n",
        "    :param scaler: Instance of MinMaxScaler for inverse transforming predictions.\n",
        "    :return: Array of predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    current_window = initial_window\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_predictions):\n",
        "            # Reshape current window to match model input\n",
        "            current_input = current_window.view(-1, sequence_length)\n",
        "            prediction = model(current_input)\n",
        "\n",
        "            # Inverse transform the prediction\n",
        "            prediction_np = prediction.numpy()\n",
        "            prediction_transformed = scaler.inverse_transform(prediction_np).flatten()\n",
        "            predictions.append(prediction_transformed)\n",
        "\n",
        "            # Update current window with new prediction\n",
        "            current_window = torch.roll(current_window, -len(selected_columns))\n",
        "            current_window[-len(selected_columns):] = torch.tensor(prediction_transformed)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Preparing the initial window from the end of 2022 data\n",
        "# Assume `last_window_2022` is extracted appropriately as a tensor\n",
        "# Example: last_window_2022 = X_test[-1] # This is just for demonstration. In practice, extract the actual last 14 days of 2022.\n",
        "\n",
        "\n",
        "# Forecasting\n",
        "n_predictions = 365  # Number of days to predict for 2023\n",
        "predictions_2023 = rolling_window_predictions(model, last_window_2022, n_predictions, scaler)\n",
        "\n",
        "print(predictions_2023)"
      ],
      "metadata": {
        "id": "_6wnSGCWszM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model prediction visualization"
      ],
      "metadata": {
        "id": "jJrm2ehmEuDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'predictions' and 'y_test_scaled' are your model's predictions and actual values, respectively, scaled back to their original range\n",
        "\n",
        "feature_names = ['Precipitation', 'Avg Hourly Temperature', 'Solar Radiation', 'Avg Hourly Pressure Station']\n",
        "test_dates = test_data['date'].reset_index(drop=True)[:len(predictions)]\n",
        "\n",
        "for i, feature_name in enumerate(feature_names):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(test_dates, y_test_scaled[:, i], label='Actual', marker='.', zorder=-1)\n",
        "    plt.plot(test_dates, predictions[:, i], label='Predicted', marker='.', zorder=1)\n",
        "    plt.title(f'{feature_name} Prediction vs Actual')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature_name)\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4o5b2D57EwPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide"
      ],
      "metadata": {
        "id": "iRKegwE7n1VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Link to weather dataset\n",
        "url_ottawa = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\"\n",
        "\n",
        "# Load and Preprocess Data\n",
        "url = url_ottawa\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# data processing transformation\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "filtered_data = data[(data['date'].dt.month >= 5) & (data['date'].dt.month <= 11)]  # restrict data range from May to November\n",
        "filtered_data = filtered_data[(filtered_data['date'].dt.year >= 2013) & (filtered_data['date'].dt.year <= 2023)]  # overall data range from years 2013-2023\n",
        "selected_columns = ['avg_hourly_temperature', 'precipitation', 'solar_radiation', 'avg_hourly_pressure_station']  # set 4 features for input and output\n",
        "final_data = filtered_data[selected_columns]\n",
        "# test data processing\n",
        "test_data = final_data[(filtered_data['date'].dt.year == 2023)] # data from 2023 only\n",
        "\n",
        "#window size\n",
        "window_size = 14\n",
        "pred_size = 1\n",
        "\n",
        "# Handle Missing Values\n",
        "#final_data = final_data.fillna(method='ffill')  # option 1: forward fill\n",
        "final_data = final_data.dropna()  # option 2: drop data with null values\n",
        "\n",
        "# Normalize the Data\n",
        "scaler = MinMaxScaler()  # MinMax scaler\n",
        "scaled_data = scaler.fit_transform(final_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "uVx_1wHCn2n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inout_sequences(input_data, tw):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    for i in range(L-tw):\n",
        "        train_seq = input_data[i:i+tw]\n",
        "        train_label = input_data[i+tw:i+tw+1]\n",
        "        inout_seq.append((train_seq ,train_label))\n",
        "    return inout_seq\n",
        "\n",
        "# Assuming 'scaled_data' is the scaled dataset from the preprocessing steps\n",
        "input_size = window_size * len(selected_columns)  # Number of features * window size\n",
        "output_size = len(selected_columns)  # Predicting the same number of features\n",
        "sequences = create_inout_sequences(scaled_data, window_size)\n"
      ],
      "metadata": {
        "id": "JsdLjVaVoklu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data (Using the first 10 years for training as per guide)\n",
        "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)\n",
        "\n",
        "# Converting sequences to DataLoader for batch processing\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_sequences, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_sequences, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model instantiation\n",
        "hidden_size = 128  # Example hidden size\n",
        "model = TDNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10  # Example epoch count\n",
        "for epoch in range(epochs):\n",
        "    for seqs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(seqs)\n",
        "        loss = criterion(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Validation step can be added here for monitoring overfitting\n"
      ],
      "metadata": {
        "id": "ymehvLxDoozB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}