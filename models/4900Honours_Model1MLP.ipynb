{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXvXqmQl9bQ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRz_V5P1VRKB"
      },
      "source": [
        "Key parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1r7lZ3UVS7X"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4  # learning rate\n",
        "window_size = 7       # window size\n",
        "pred_size = 1         # prediction step\n",
        "num_features = 4      # number of features per vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq_MFCBW-i1V"
      },
      "outputs": [],
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "  \"\"\"\n",
        "    Convert time series data into a supervised learning format.\n",
        "\n",
        "    :param data: Time series data (list or numpy array).\n",
        "    :param n_in: Number of lag observations as input (default is 1).\n",
        "    :param n_out: Number of future observations as output (default is 1).\n",
        "    :param dropnan: Whether to drop rows with NaN values (default is True).\n",
        "    :return: DataFrame with lagged and forecasted observations.\n",
        "    \"\"\"\n",
        "  n_vars = 1 if type(data) is list else data.shape[1]\n",
        "  df = pd.DataFrame(data)\n",
        "  cols, names = list(), list()\n",
        "  # Input sequence (t-n, ... t-1)\n",
        "  for i in range(n_in, 0, -1):\n",
        "      cols.append(df.shift(i))\n",
        "      names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "  # Forecast sequence (t, t+1, ... t+n_out)\n",
        "  for i in range(0, n_out):\n",
        "      cols.append(df.shift(-i))\n",
        "      if i == 0:\n",
        "          names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "      else:\n",
        "          names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "  agg = pd.concat(cols, axis=1)\n",
        "  agg.columns = names\n",
        "  # Drop rows with NaN values\n",
        "  if dropnan:\n",
        "      agg.dropna(inplace=True)\n",
        "  return agg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0blknYJ3jL5"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaTrINOi-2zd"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYfNupGr_D-I"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def get_predictions_and_targets(loader, model):\n",
        "  \"\"\"\n",
        "    Get predictions and corresponding targets from a PyTorch model using a data loader.\n",
        "\n",
        "    :param loader: Data loader providing batches of input data and labels.\n",
        "    :param model: Trained PyTorch model.\n",
        "    :return: Numpy arrays containing predictions and corresponding targets.\n",
        "    \"\"\"\n",
        "  model.eval()\n",
        "  predictions, targets = [], []\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in loader:\n",
        "          outputs = model(inputs)\n",
        "          predictions.append(outputs.numpy())\n",
        "          targets.append(labels.numpy())\n",
        "  predictions = np.vstack(predictions)\n",
        "  targets = np.vstack(targets)\n",
        "  return predictions, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3EgzROw_ZD2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_X, test_y, train_loader, test_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_X)\n",
        "        test_loss = criterion(predictions, test_y)\n",
        "    print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "    train_predictions, train_targets = get_predictions_and_targets(train_loader, model)\n",
        "    test_predictions, test_targets = get_predictions_and_targets(test_loader, model)\n",
        "\n",
        "    train_mae = mean_absolute_error(train_targets, train_predictions)\n",
        "    train_mse = mean_squared_error(train_targets, train_predictions)\n",
        "    test_mae = mean_absolute_error(test_targets, test_predictions)\n",
        "    test_mse = mean_squared_error(test_targets, test_predictions)\n",
        "\n",
        "    print(f\"Training MAE: {train_mae}, Training MSE: {train_mse}\")\n",
        "    print(f\"Testing MAE: {test_mae}, Testing MSE: {test_mse}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0csph7oV3pgQ"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVK3LvnX9fLY",
        "outputId": "b8c6b6ba-9ac9-456a-bee8-274613e2a77a"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load the dataset\n",
        "    url = 'https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv'\n",
        "    data = pd.read_csv(url)\n",
        "\n",
        "    data['date'] = pd.to_datetime(data['date'])\n",
        "    # Interpolate missing solar radiation values\n",
        "    data['solar_radiation'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "    # Filter data for dates between May 1st and November 30th for each year\n",
        "    filtered_data = data[(data['date'].dt.month >= 5) & (data['date'].dt.month <= 11)]\n",
        "    data_filtered = data[(data['date'].dt.year >= 2015) & (data['date'].dt.year <= 2022)]\n",
        "\n",
        "    # Select the required columns and preprocess\n",
        "    columns_required = ['avg_hourly_temperature', 'precipitation', 'avg_hourly_pressure_station', 'solar_radiation']\n",
        "    data_filtered = data_filtered[columns_required].dropna()\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data_scaled = scaler.fit_transform(data_filtered)\n",
        "\n",
        "    # Convert to supervised learning problem with a window size of 3DAYS\n",
        "    data_supervised = series_to_supervised(data_scaled, window_size, pred_size)  ##7          ####################################\n",
        "\n",
        "    # Split the dataset\n",
        "    n_obs = window_size * num_features  # Update for 3DAYS window size// and 4 features     #############################\n",
        "    values = data_supervised.values\n",
        "    n_train_days = int(len(values) * 0.8)\n",
        "    train = values[:n_train_days, :]\n",
        "    test = values[n_train_days:, :]\n",
        "    train_X, train_y = train[:, :n_obs], train[:, -num_features:]\n",
        "    test_X, test_y = test[:, :n_obs], test[:, -num_features:]\n",
        "\n",
        "    # Convert to tensors\n",
        "    train_X = torch.tensor(train_X, dtype=torch.float32)\n",
        "    train_y = torch.tensor(train_y, dtype=torch.float32)\n",
        "    test_X = torch.tensor(test_X, dtype=torch.float32)\n",
        "    test_y = torch.tensor(test_y, dtype=torch.float32)\n",
        "\n",
        "    # DataLoader\n",
        "    train_dataset = TensorDataset(train_X, train_y)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_dataset = TensorDataset(test_X, test_y)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Initialize the model with the updated input dimension\n",
        "    #model = MLP(12, 4)\n",
        "    model = MLP(n_obs, num_features)                                     # Update for 3DAYS window size 12/// 7days 28/// 30DAYS 120\n",
        "    criterion = nn.MSELoss()                                     ##########################################\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 50\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "    evaluate_model(model, test_X, test_y, train_loader, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBm1zc3Cltrh"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Epoch 50/50, Loss: 0.009859252721071243\n",
        "Test Loss: 0.01037069596350193\n",
        "Training MAE: 0.07271228730678558, Training MSE: 0.011210594326257706\n",
        "Testing MAE: 0.07027967274188995, Testing MSE: 0.01037069596350193\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWgK5e78PsR1"
      },
      "source": [
        "**Prediction with recursive forcasting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLkgtzezP0AN"
      },
      "outputs": [],
      "source": [
        "def rolling_window_predictions(model, initial_window, scaler, n_predictions):\n",
        "    \"\"\"\n",
        "    Predict future values using a rolling window approach.\n",
        "\n",
        "    :param model: Trained PyTorch model for making predictions.\n",
        "    :param initial_window: The last known window of features to start making predictions from.\n",
        "    :param scaler: Fitted MinMaxScaler instance used for inverse transforming the predictions.\n",
        "    :param n_predictions: Number of future time steps to predict.\n",
        "    :return: Array of predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    current_window = initial_window.clone().detach()  # Ensure it's a tensor.\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation for inference.\n",
        "        for _ in range(n_predictions):\n",
        "            # Make a prediction using the current window.\n",
        "            prediction = model(current_window).numpy()  # Model output to numpy array.\n",
        "            # Inverse scale the prediction to the original scale.\n",
        "            prediction = scaler.inverse_transform(prediction).flatten()\n",
        "            # Store the prediction.\n",
        "            all_predictions.append(prediction)\n",
        "\n",
        "            # Update the window for the next prediction.\n",
        "            # Convert the prediction back to a tensor and reshape for concatenation.\n",
        "            prediction_tensor = torch.tensor(scaler.transform(prediction.reshape(1, -1)), dtype=torch.float32)\n",
        "            # Roll the window to remove the oldest day and insert the new prediction at the end.\n",
        "            current_window = torch.roll(current_window, -num_features, dims=1)\n",
        "            current_window[:, -num_features:] = prediction_tensor[:, :num_features]  # Assuming 4 features per day.\n",
        "\n",
        "    return np.array(all_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHXd7tRizppx"
      },
      "source": [
        "**Predicting for next two weeks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsNK47voeIAn",
        "outputId": "29f29e99-7954-410c-cd07-37af8f82b9f3"
      },
      "outputs": [],
      "source": [
        "data_2023 = data[(data['date'].dt.year == 2023) & (data['date'].dt.month >= 5) & (data['date'].dt.month <= 11)]\n",
        "# Ensure data is sorted by date\n",
        "data_2023 = data_2023.sort_values(by='date')\n",
        "data_2023_filtered = data_2023[columns_required]\n",
        "# Apply scaling using the already fitted scaler\n",
        "data_2023_scaled = scaler.transform(data_2023_filtered)\n",
        "\n",
        "\n",
        "# Extract the first 3 days (rows) as the initial window for the prediction\n",
        "initial_window = data_2023_scaled[:window_size].reshape(1, -1)                             #set window size\n",
        "initial_window = torch.tensor(initial_window, dtype=torch.float32)\n",
        "\n",
        "\n",
        "n_future_steps = 210\n",
        "# Generate predictions\n",
        "future_predictions = rolling_window_predictions(model, initial_window, scaler, n_future_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R3hWDqHknc9t",
        "outputId": "1c33269c-8e8b-43f1-a572-e93bd12162ab"
      },
      "outputs": [],
      "source": [
        "# Graph drawing\n",
        "#start_day = 1  # 1 is May 1st\n",
        "num_days = 210  # Change to see number of days for prediction. Must be <= n_future_steps above\n",
        "\n",
        "#pd.to_datetime()\n",
        "\n",
        "actual_2023 = data_2023.set_index('date')['2023-05-01':'2023-11-30']\n",
        "actual_values = actual_2023[columns_required]\n",
        "# Generate the dates corresponding to each prediction\n",
        "#prediction_dates = pd.date_range(start='2023-05-01', periods=n_future_steps)\n",
        "prediction_dates = pd.date_range(start='2023-05-01', periods=num_days)\n",
        "\n",
        "# Convert the predictions array into a DataFrame with the prediction dates\n",
        "predicted_values = pd.DataFrame(future_predictions[:num_days], index=prediction_dates, columns=columns_required)\n",
        "# Combine actual and predicted values\n",
        "comparison_df = actual_values.join(predicted_values, rsuffix='_predicted')\n",
        "\n",
        "# Ensure both DataFrames are aligned by date\n",
        "comparison_df_aligned = comparison_df.dropna()\n",
        "\n",
        "# Recalculate MAE and RMSE using the aligned DataFrame\n",
        "for feature in columns_required:\n",
        "    actual = comparison_df_aligned[feature]\n",
        "    predicted = comparison_df_aligned[f'{feature}_predicted']\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "    print(f'{feature} - MAE: {mae}, RMSE: {rmse}')\n",
        "\n",
        "\n",
        "# Visualization using the aligned DataFrame\n",
        "for feature in columns_required:\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(comparison_df_aligned.index, comparison_df_aligned[feature], label='Actual', marker='o')\n",
        "    plt.plot(comparison_df_aligned.index, comparison_df_aligned[f'{feature}_predicted'], label='Predicted', marker='x', alpha=0.7)\n",
        "    plt.title(f'Actual vs. Predicted {feature}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
