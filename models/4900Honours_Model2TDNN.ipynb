{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuyvzA-awUcN"
      },
      "source": [
        "Weather Forecasting with 4 features using historical data via TDNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8ohxmS2wM6E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma66IogDUEHr"
      },
      "source": [
        "Key parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx5E7HoRUF5i"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4  # learning rate\n",
        "window_size = 7       # window size\n",
        "pred_size = 1         # prediction step\n",
        "num_features = 4      # number of features per vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78fnfBGJwYxw"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "\n",
        "class TDNN(nn.Module):\n",
        "    def __init__(self, input_size, window_size, output_size):\n",
        "        super(TDNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(32 * window_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwkWPcblwbN4"
      },
      "outputs": [],
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # Input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # Forecast sequence (t, t+1, ... t+n_out)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # Put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # Drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGqdJvL26fU0"
      },
      "outputs": [],
      "source": [
        "# Function to Create Sequences\n",
        "def create_sequences(data, window_size=10):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        sequences.append(data[i:i+window_size]) # 1..10, 2..11, 3..12, ......\n",
        "        labels.append(data[i+window_size])\n",
        "    return np.array(sequences), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1byiuadUAum8",
        "outputId": "f4c174c9-7cfe-4990-b9ce-35e47df93638"
      },
      "outputs": [],
      "source": [
        "# Link to 6 weather datasets\n",
        "url_ottawa = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawa_daily.csv\"\n",
        "url_ottawa_south = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_ottawasouth_daily.csv\"\n",
        "#url_gatineau = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_gatineau_daily.csv\"\n",
        "#url_chelsea = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_chelsea_daily.csv\"\n",
        "#url_kemptville = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_kemptville_daily.csv\"\n",
        "#url_renfrew = \"https://raw.githubusercontent.com/noobstang/NNtraining/master/Weather49Sets/weatherstats_renfrew_daily.csv\"\n",
        "\n",
        "# Load data\n",
        "url = url_ottawa\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Convert date and filter data\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "filtered_data = data[(data['date'].dt.month >= 5) & (data['date'].dt.month <= 11)]\n",
        "filtered_data = filtered_data[(filtered_data['date'].dt.year >= 2013) & (filtered_data['date'].dt.year <= 2023)]\n",
        "\n",
        "# Select relevant columns\n",
        "selected_columns = ['avg_hourly_temperature', 'precipitation', 'solar_radiation', 'avg_hourly_pressure_station']\n",
        "filtered_data_train = filtered_data[(filtered_data['date'].dt.year < 2023)]\n",
        "filtered_data_test = filtered_data[(filtered_data['date'].dt.year == 2023)]\n",
        "#final_data = filtered_data[selected_columns]\n",
        "final_data = filtered_data_train[selected_columns]\n",
        "final_data_test = filtered_data_test[selected_columns]\n",
        "\n",
        "# Handle Missing Values\n",
        "#final_data = final_data.fillna(method='ffill')  # option 1: forward fill\n",
        "final_data = final_data.dropna()  # option 2: drop data with null values\n",
        "final_data_test = final_data_test.dropna()\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(final_data)\n",
        "#scaled_data = pd.DataFrame(scaled_data, columns=final_data.columns)\n",
        "\n",
        "# do the same for test data\n",
        "scaled_data_test = scaler.fit_transform(final_data_test)\n",
        "#scaled_data_test = pd.DataFrame(scaled_data_test, columns=final_data_test.columns)\n",
        "\n",
        "#scaled_data_test = scaler.fit_transform(final+)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "#train_data = scaled_data[(filtered_data['date'].dt.year < 2023)]\n",
        "#test_data = scaled_data[(filtered_data['date'].dt.year == 2023)]\n",
        "train_data = scaled_data\n",
        "test_data = scaled_data_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coI1mH0IyL-_"
      },
      "outputs": [],
      "source": [
        "# Prepare data for model input\n",
        "\n",
        "#input_data = series_to_supervised(train_data.values, n_in=window_size, n_out=pred_size)\n",
        "input_data = series_to_supervised(train_data, n_in=window_size, n_out=pred_size)\n",
        "\n",
        "input_features = input_data.iloc[:, :-num_features].values\n",
        "output_features = input_data.iloc[:, -num_features:].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M96psJe_mN2"
      },
      "outputs": [],
      "source": [
        "# Adjust series_to_supervised output to correct shape for Conv1d\n",
        "def reshape_input_for_conv1d(input_features, window_size, num_features):\n",
        "    # Reshape from (samples, features*window_size) to (samples, num_features, window_size)\n",
        "    #return input_features.reshape(-1, window_size, num_features)\n",
        "    return input_features.reshape(-pred_size, window_size, num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-aaRtplz9Ch"
      },
      "outputs": [],
      "source": [
        "#num_features = 4\n",
        "# Reshape input_features for the Conv1D layer\n",
        "input_features_reshaped = reshape_input_for_conv1d(input_features, window_size, num_features)\n",
        "\n",
        "# Convert datasets to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(input_features_reshaped, dtype=torch.float32)\n",
        "#X_train_tensor = torch.tensor(input_features, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(output_features, dtype=torch.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8nTiz1q1whz"
      },
      "outputs": [],
      "source": [
        "#print(X_train_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmdvepXKwcaR",
        "outputId": "259828f4-cc3b-4951-eb06-65c57c041cbe"
      },
      "outputs": [],
      "source": [
        "# Convert datasets to PyTorch tensors\n",
        "#X_train_tensor = torch.tensor(input_features, dtype=torch.float32)\n",
        "#y_train_tensor = torch.tensor(output_features, dtype=torch.float32)\n",
        "\n",
        "# DataLoader setup\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Model initialization\n",
        "#model = TDNN(input_size=4, window_size=window_size, output_size=4)\n",
        "model = TDNN(input_size=num_features, window_size=window_size, output_size=num_features)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # default: 0.001 (1e-4)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sPJ4THCzwdcY",
        "outputId": "344723f3-a55d-4f1a-cd65-5d97c9934aa1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Predicting recursively for 2023\n",
        "model.eval()\n",
        "test_input = test_data[:window_size]  # first two weeks of 2023\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(test_data) - window_size):\n",
        "    test_input_tensor = torch.tensor(test_input.reshape(1, window_size, -1), dtype=torch.float32)  # need to modify if pred_size is changed\n",
        "    with torch.no_grad():\n",
        "        pred = model(test_input_tensor)\n",
        "        pred = pred.numpy()\n",
        "    predictions.append(pred[0])\n",
        "    # Slide window by removing first and adding predicted\n",
        "    test_input = np.vstack([test_input[1:], pred])\n",
        "\n",
        "# Calculate errors\n",
        "predictions = np.array(predictions)\n",
        "true_values = test_data[window_size:]\n",
        "\n",
        "# Inverse transform the scaled predictions and actual values\n",
        "predictions_inverse = scaler.inverse_transform(predictions)\n",
        "true_values_inverse = scaler.inverse_transform(true_values)\n",
        "\n",
        "# Calculate errors on the original scale\n",
        "mae = mean_absolute_error(true_values_inverse, predictions_inverse)\n",
        "rmse = math.sqrt(mean_squared_error(true_values_inverse, predictions_inverse))\n",
        "\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Square Error: {rmse:.4f}')\n",
        "\n",
        "# Optional: plotting the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize arrays to hold MAE and RMSE values\n",
        "maes = []\n",
        "rmses = []\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(['Temperature', 'Precipitation', 'Solar Radiation', 'Pressure']):\n",
        "    plt.subplot(4, 1, i+1)\n",
        "    plt.plot(true_values_inverse[:, i], label='Actual')\n",
        "    plt.plot(predictions_inverse[:, i], label='Predicted')\n",
        "    plt.title(feature)\n",
        "    plt.legend()\n",
        "\n",
        "    # Calculate MAE and RMSE for this feature\n",
        "    mae = mean_absolute_error(true_values_inverse[:, i], predictions_inverse[:, i])\n",
        "    rmse = math.sqrt(mean_squared_error(true_values_inverse[:, i], predictions_inverse[:, i]))\n",
        "    maes.append(mae)\n",
        "    rmses.append(rmse)\n",
        "\n",
        "    # Print MAE and RMSE for this feature\n",
        "    print(f'{feature} - Mean Absolute Error: {mae:.4f}')\n",
        "    print(f'{feature} - Root Mean Square Error: {rmse:.4f}')\n",
        "#plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bPZTxc5ENLK2",
        "outputId": "0f1eed6b-3358-42d4-f122-004f7104d167"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Assuming 'scaler' is your MinMaxScaler instance and you've already calculated 'predictions_inverse' and 'true_values_inverse'\n",
        "# Also assuming predictions start from May and 'predictions_inverse' has already been sliced to only include prediction period from May to November\n",
        "\n",
        "# Initialize arrays to hold MAE and RMSE values for the zoomed period\n",
        "maes_zoomed = []\n",
        "rmses_zoomed = []\n",
        "\n",
        "# Plotting the results for the first 60 days and calculating errors for each feature\n",
        "plt.figure(figsize=(15, 10))\n",
        "features = ['Temperature', 'Precipitation', 'Solar Radiation', 'Pressure']\n",
        "for i, feature in enumerate(features):\n",
        "    plt.subplot(4, 1, i + 1)\n",
        "    plt.plot(true_values_inverse[:14, i], label='Actual ' + feature)\n",
        "    plt.plot(predictions_inverse[:14, i], label='Predicted ' + feature)\n",
        "    plt.title(feature + \" (First 14 Days)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Calculate MAE and RMSE for this feature for the first 60 days\n",
        "    mae_zoomed = mean_absolute_error(true_values_inverse[:14, i], predictions_inverse[:14, i])\n",
        "    rmse_zoomed = sqrt(mean_squared_error(true_values_inverse[:14, i], predictions_inverse[:14, i]))\n",
        "    maes_zoomed.append(mae_zoomed)\n",
        "    rmses_zoomed.append(rmse_zoomed)\n",
        "\n",
        "    # Print MAE and RMSE for this feature for the first 60 days\n",
        "    print(f'{feature} (First 14 Days) - Mean Absolute Error: {mae_zoomed:.4f}')\n",
        "    print(f'{feature} (First 14 Days) - Root Mean Square Error: {rmse_zoomed:.4f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optionally, print all zoomed MAEs and RMSEs\n",
        "print(\"Zoomed-in MAEs:\", maes_zoomed)\n",
        "print(\"Zoomed-in RMSEs:\", rmses_zoomed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI5tfAKIcyfa",
        "outputId": "05649cbc-3f58-4d25-c946-ba0502dbd52f"
      },
      "outputs": [],
      "source": [
        "# Initialize arrays to hold MAE and MSE values for the first 14 days\n",
        "maes_14_days = []\n",
        "mses_14_days = []\n",
        "\n",
        "# Calculate errors for each feature for the first 14 days\n",
        "features = ['Temperature', 'Precipitation', 'Solar Radiation', 'Pressure']\n",
        "print(\"Errors for the first 14 days:\")\n",
        "for i, feature in enumerate(features):\n",
        "    # Calculate MAE and MSE for this feature for the first 14 days\n",
        "    mae_14_days = mean_absolute_error(true_values_inverse[:14, i], predictions_inverse[:14, i])\n",
        "    mse_14_days = mean_squared_error(true_values_inverse[:14, i], predictions_inverse[:14, i])\n",
        "    maes_14_days.append(mae_14_days)\n",
        "    mses_14_days.append(mse_14_days)\n",
        "\n",
        "    # Print MAE and MSE for this feature for the first 14 days\n",
        "    print(f'{feature} - Mean Absolute Error: {mae_14_days:.4f}')\n",
        "    print(f'{feature} - Mean Squared Error: {mse_14_days:.4f}')\n",
        "\n",
        "# Optionally, if you want to see the root mean square error, you can calculate it from the MSE:\n",
        "rmses_14_days = [sqrt(mse) for mse in mses_14_days]\n",
        "print(\"Root Mean Squared Errors for the first 14 days:\", rmses_14_days)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
